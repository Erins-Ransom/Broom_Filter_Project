\documentclass[../paper.tex]{subfiles}

\begin{document} Bender et al. \cite{broom-filter} introduce a different adaptive filter, called the ``broom filter''
(because it cleans up after itself). They make two key contributions to the
field of adaptive filters: describing the broom filter data structure itself,
as well as formalizing the idea of adaptivity and providing proofs regarding the guaratees
of the broom filter, and adaptive AMQs in general. The broom filter builds on
a previous AMQ called the quotient filter \cite{quotient-filter}, and, like the adaptive
cuckoo filter, consists of both a local and remote representation. In fact, one
of the main theoretical results they obtain shows that any adaptive AMQ must use
some remote (larger) storage in order to maintain adaptivity. Unfortunately, from
our experience the broom filter is quite impractical. Even a naive implementation
would be cumbersome and time-consuming to write, and important details such as
how adaptivity bits can be stored and maintained efficiently are left out.

\subsection{The Quotient Filter}

The broom filter builds on a previous work called the quotient filter, which
is a non-adaptive AMQ meant to be an optimal and practical alternative to a bloom filter.
The quotient filter uses a single hash function and computes a \textit{fingerprint} for
each element inserted to the set. Given an element $x$, the fingerprint of $x$ is a
prefix of the hash $h(x)$ and is divided into two separate pieces, the \textit{quotient} with
size $q$ bits and the \textit{remainder}, with size $r$ bits. The fingerprint is therefore
the first $q+r$ bits of $h(x)$.
The quotient filter is an array of $2^q$ buckets, where
each bucket consists of $r$ bits, plus three extra metadata bits.
The sizes of the quotient and remainder are tunable
parameters of the quotient filter, which affect the false-positive probability $\epsilon$.

\textbf{Insertions and lookups:} To insert an element $x$ into the quotient filter, we first
compute the fingerprint of $x$: $p(x)$. Then we find the bucket corresponding
to the quotient and insert the remainder bits at that bucket. Assuming the bucket
is empty during insertion, a lookup for $y$ would be simple: we go to the correct bucket as given
by the quotient of $y$ and check if the remainder bits are the same. However, if the bucket is
full, linear probing is used to find an empty bucket, provided two invariants are maintained:

\begin{enumerate}
    \item All remainders with the same quotient are stored contiguously in a \textit{run}, wrapping around to the beginning if necessary.
    \item If remainder $a$ is stored before remainder $b$, then the quotient of $a$ is less than or equal to the quotient of $b$ modulo the wrapping.
\end{enumerate}

We then have to apply the correct metadata bits to all buckets that are touched by the probe. These metadata
bits will allow us to determine which remainders correspond to which quotients,
since due to linear probing remainders will be shifted from their correct quotients.
The three metadata bits provide the following information for each bucket:

\begin{enumerate}
    \item \texttt{occupied} bit: is the bucket occupied?
    \item \texttt{shifted} bit: is the remainder stored at this bucket shifted from its corresponding quotient?
    \item \texttt{run} bit: is this bucket a part of a run?
\end{enumerate}

    Whenever a remainder is
    added, the \texttt{occupied} bit for its intended bucket is always set to 1.  If
    a remainder is not stored in its intended bucket, then the \texttt{shifted} bit
    of the bucket it is stored in is set to 1.  If the remainder has the same
    intended bucket as the remainder immediately before it, the \texttt{run} bit is set to 1.  When we
    perform a lookup for element $x$, we check the bucket indexed by the first
    $q$ bits of $h(x)$.  If \texttt{occupied} is 0, we immediately return
    \textit{absent}; otherwise, if \texttt{shifted} is 0, we scan the current run for a remainder
    that matches the remainder being looked up, and return \textit{present} if a match is found,
    and \textit{absent} otherwise.
    If the \texttt{shifted} bit is 1, then we must
    first find the beginning of the {\bf cluster}, a group of runs with no
    empty buckets between them, and then scan forward keeping track of the
    number of runs and the number of occupied buckets to determine the location
    of our target run.  If we reach the end of the cluster without finding it,
    then we return $false$, but if we do find the run, we compare to the
    remainders in that run and return $true$ or $false$ accordingly.  Deletes
    can be performed by looking up the corresponding remainder (of which their
    may be multiple) and removing it and then shifting other remainders and
    updating metadata bits as needed.
    
    Let $m$ be the number of occupied buckets in the filter (the number of elements that have been inserted so far). A false positive thus occurs if $h(x) =
    h(y)$ when $x \in S$ and $y \notin S$.  Assuming $h: {\bf U} \rightarrow
    \{0, ... , 2^{q+r}-1\}$ generates outputs that are distributed uniformly
    and independently, then the probability of a false positive is given by $$
    1 - \left(1 - \frac{1}{2^{q+r}}\right)^m \approx 1- e^{-m/(q+r)} \leq
    \frac{m}{2^{(q+r)}} \leq \frac{2^q}{2^{(q+r)}} \leq 2^{-r}.$$

    If we would like to choose $r$ and $q$ such that the filter has a false-positive
    probability of at most $\epsilon$, and $n$ is the maximum set size, we can choose $q=\log(n)$
    and $r=\log(1/\epsilon)$.
    With these choices, the quotient filter achieves a false-positive probability of at most
    $2^{-r} = \epsilon$ while using $O(2^q r) = O(n \log (1/\epsilon))$ space.  	

\subsection{Adding Adaptivity}

The broom filter is a provably adaptive AMQ which makes use of quotient filters
while still providing the guarantees about the false-positive probability for
a sequence of possibly non-independent queries. The broom filter consists
of both a local and remote structure. The local partition is a fully functioning
AMQ on its own, but the remote partition is necessary to provide additional
information for adapting to false positives. Specifically, whenever there is
a false-positive caused by some element $x \notin S$, the remote structure
should provide the element $y \in S$ that caused the false-positive so
that we can modify its representation in the local AMQ so that future lookups
of $x$ will no longer result in false positives. The specific implementation
of the remote structure is not detailed, but we provide some ideas for
how it could be implemented in a later section. The need to access a remote
portion of the filter may seem counter-intuitive given that AMQs are designed
to avoid remote lookups, but the AMQ will only ever access the remote structure
when it would have been accessed anyway (to look up an item in the dictionary or
to insert an item to the dictionary).

The details of the local representation are presented here and are tuned to
improve lookup and insert performance, rather than for enabling adaptivity,
so we only give a brief overview here.
Let $n$ be the maximum set size that will be stored by the broom filter, and $\epsilon$
be the false-positive probability of a single query.
The local representation of the broom filter consists of two quotient filters, both
with remainders of size $r=\log(1/\epsilon)$. The designs of these quotient filters
differs based on the size of the remainders. In the \textbf{small remainder} case,
where $r \leq 2 \log \log n$, both levels are quotient filters with independent hash
functions. The first layer contains $O(n)$ buckets and the second layer contains
$O(\log n/n)$ buckets. In the \textbf{large remainder} case, where $r > 2 \log \log n$,
only a single hash function is used and backyard hashing \cite{backyard-hashing} is used
to differentiate them.

The broom filter uses hash functions $h : {\bf U} \rightarrow \{0, ..., n^c\}$, for
some constant $c \geq 4$. The constant $c$ must ensure that the target set of the hash
function is large enough that the hash function is perfect and cannot cause elements
to collide. However since only a subset of the bits from the hash are used in the local
AMQ this doesn't cause space problems.

%  Suffice to say, both representations
%     require $O(n\log(1/\epsilon)$ space as well as $O(n)$ space for the
%     adaptivity bits.  

In the broom filter, the \textbf{fingerprint} of an element $x$ is comprised of the first
$q + r + a$ bits of the hash of $x$, where $q$ is the number of quotient bits, $r$ is the
number of remainder bits, and $a$ is the number of \textit{adaptivity} bits. The number of
adaptivity bits can vary dynamically and can be different for each element in the set. We
also maintain an invariant regarding fingerprints to make extending adaptivity bits possible.

\begin{itemize}
    \item[] \textbf{Invariant:} No fingerprint is a prefix of another fingerprint.
\end{itemize}

The adaptivity bits of a fingerprint may be extended in two cases:

\begin{enumerate}
    \item When a false-positive occurs from a lookup.
    \item When a fingerprint is added to the broom filter.
\end{enumerate}

Note that in both cases, we must access the remote structure regardless of adaptivity. A false-positive
caused by the lookup causes a lookup in the remote dictionary, and insertion requires inserting
to the remote dictionary to maintain its state correctly. Therefore in these cases we can
acquire some additional information from the remote structure for ``free'' in the sense that
the remote access is the expensive operation and must be done anyway.

To check if an item is in the set, we check if the fingerprint at each bucket is a prefix
of the item's hash, and if so, we return \textit{present}. 
If a false positive is caused by some element $x \notin S$, this means that
the fingerprint of some element $y$ in the set is a prefix of $h(x)$. If we
simply extend the fingerprint of $y$ to include more adaptivity bits until
it is no longer a prefix of $h(x)$ then $x$ will no longer cause a false positive. Extending
the fingerprint of $y$ requires accessing the full hash of $y$, which can only be
provided by the remote dictionary. Luckily, our invariant ensures that there is
only one element in the dictionary with $y$'s fingerprint, so we can find the unique
$y$.

During insertion to the broom filter, we must make sure that the invariant is maintained.
We do so by checking the newly added fingerprint against all other fingerprints with
the same quotient. If any of those fingerprints share the same remainder, adaptivity bits are
added to both fingerprints until neither is a prefix of the other (so the invariant
is maintained). Extending the fingerprints that already exist in the AMQ requires using
the same query in the remote dictionary as during the lookup described above. Note that additional
adaptivity bits may be added in this step to handle deletions as well, but this will be
explained in a later section.

     If we ignore deletes, then what we have already described is sufficient to
     achieve adaptivity since no a lookup for any given element will never
     result in a false positive more than once.  That said, the Broom filter
     does support deletes and we should also be concerned about the continued
     addition of adaptivity bits to the local representation.  Before we
     address these points, it is worth mentioning that the maintenance of this
     invariant and the adaptivity of the filter is dependent on the hash
     function being one-to-one.  Consider if $x \notin S$, $y \in S$ and  $h(x)
     = h(y)$.  In such a case, adding adaptivity bits to the fingerprint of $y$
     will never make it not a prefix of $h(x)$.  This could potentially be
     resolved by allowing fingerprints to grow beyond the size of their hash,
     but this would likely have problematic implications for the size of the
     local representation.  This does not appear to be addressed in the design
     of the Broom Filter.  Deletes in the Broom Filter are carried out like
     those in a Quotient Filter except that the quotient and the adaptivity
     bits are not removed but are left as a {\bf ghost} in the filter.  When a
     new fingerprint is added, if there is a matching ghost in the filter, then
     the new fingerprint takes on all the adaptivity bits of the ghost even  if
     this is more than required for the standard insert process.  What this
     achieves is that false positives can no longer be generated by timing
     attacks involving repeatedly adding and removing an element from the
     filter that collides with some element not in the filter.  This is
     because, a re-inserted element essentially retains its adaptivity bits
     from before it was removed and thus will not collide with any elements
     that collided with it in the past.   
	 
     Finally, we must address the issue of continually growing adaptivity bits.
     This is done with a deamortized equivalent of rebuilding the the filter
     every $\Theta (n)$ times adaptivity bits are added.  This is done by
     keeping two hash functions, $h_a$ and $h_b$, and having phases that
     gradually switch between hash functions.  At the beginning of a phase,
     frontier $z = -\infty$ and only a $h_a$ is used.  Once adaptivity bits are
     added, the smallest constant $k > 1$ elements of $S$ that are greater than
     $z$ are deleted from the filter (including their ghosts) and re-inserted
     using $h_b$, after which $z$ is set to be the largest element that was
     re-inserted.  This requires a access to the remote representation, but
     again this will only happen when the underlying dictionary is already
     being accessed.  When an element is $x$ looked up or inserted, $h_a$ is
     used if $x > z$ and $h_b$ is used otherwise.  Once $z$ reaches the largest
     element in $S$, then a new phase begins.  This is then enough to guarantee
     that, with high probability, there are $O(n)$ adaptivity bits in the
     filter at any given time.  The Broom Filter thus achieves adaptivity while
     still only using $O(n \log (1/\epsilon)$ space locally.  


\subsection{Implementation details}

Bender et al. \cite{broom-filter} provide a thorough description of the theory of
the broom filter, but some key implementation details are left out.
Most notably the storage of the adaptivity bits is a real problem
in practice because they are variable sized chunks of memory that
are often smaller than a machine word. This means that in practice,
to store the adaptivity bits, we also need to keep track of sizes
for each chunk, and need to dynamically resize when more bits are needed.
This is a lot of overhead, especially in real applications the adaptivity
bits don't use much space in the first place (a universe of 64-bit integers,
which is common for database join operations). Where to store the adaptivity
bits is also unclear. We need adaptivity bits for every element in
the set, but since our set is stored as a quotient filter, unless
we want to be constantly resizing the entire quotient filter, the
adaptivity bits should be stored in a separate memory. In general
the concept of adaptivity bits is impractical because computers perform
well when working with constant, repetitive data, which is the opposite
of what is needed to implement adaptivity bits.

Another key detail that proves impractical is the RevLookup operation
needed in the remote data structure for extending the adaptivity bits. When
an insertion happens, there may be fingerprints stored in the broom filter
which are prefixes of the item being inserted. In that case the adaptivity
bits of the colliding element need to be extended until they are no longer
a prefix of the hash being inserted. To extend the adaptivity bits, we
need the full hash of the colliding element, but since we only have its
fingerprint stored in the broom filter, this will require a lookup in
the remote storage. Efficiently supporting this lookup requires additionally
maintaing a table of fingerprints to full hashes in the remote data structure
which is quite costly because every time a fingerprint is extended in the
broom filter, the same fingerprint needs to be extended in the remote table
(which will require re-hashing the fingerprint if the remote table is maintained
as a hash table).

Other difficulties with implementation arise with the hash function. Bender et al. \cite{broom-filter}
mention that the hash function must map $U \rightarrow \{0,\ldots,n^c\}$ where
$n$ is the maximum set size and $c$ is a constant greater than 3. If the universe
is larger than $n^c$ then it is possible to have a complete collision between
$h(x)$ and $h(y)$. This has a low probability of occurring but if it does
occur and the colliding element is repeatedly queried, then the false positive
rate will be close to 1, causing the adaptivity of the filter to fall apart. Therefore
our hash function must hash from the universe to something larger than the universe
and be a perfect hash function to avoid these collisions. This immediately rules
out using the broom filter for strings because in that case the universe is too large.
In addition, with a universe of 64-bit integers, we have to use a hash function
that maps to integers larger than 64 bits (unless we want a maximum set size capped
at $2^{16}$ which is fairly small), and maintaining integers larger than the computer's
word size is impractical.

\subsection{A Lower Bound for Adaptivity} While we may have found the
implementation of the Broom Filter to be impractical in practice, the paper's theory
section offers an important insight in designing adaptive filters. As part of the Broom Filter's design,
Bender et al. also prove that any AMQ storing a set of size $n$ from a universe
of size $u > n^4$ requires $\Omega (\min \{n\log n, n\log\log u\})$ bits of
space with high probability in order to maintain a sustained false-positive
rate $\epsilon < 1$.  This is why the remote representation is required if the
local representation is to be near the optimal size for a traditional AMQ and
why it is not surprising that the Adaptive Cuckoo Filter also shares this two
part structure.  A similar proof was explored with the {\it Bloomier Filter} 
\cite{bloomier-filter} in assessing the space required when dynamically 
updating it's whitelist. The proof makes use of adversarial model where the 
goal of the adversary is to adaptively generate a sequence of $O(n)$ lookups 
that force the AMQ to either use too much space or to fail to sustain its 
false-positive rate of $\epsilon$.  
	
	{\bf The Adversary's Attack:} The adversary starts with a set $S$ of size $n$ chosen
	uniformly at random form $U$.  The attack then proceeds in rounds.  For each round, 
	a set $Q$ of size $n$ is chosen uniformly at random from $U\setminus S$.  All members
	of $Q$ are then looked up. On each subsequent round, any false-positives from the 
	previous round are queried again. 
	
	The general idea of the proof is that, we expect the adversary to find a certain 
	fraction of false-positives which is shown to have a high concentration using Chernoff
	bounds.  Therefore, with high probability, the AMQ will have to fix an $\epsilon$ fraction
	of false-positives in each round or fail to sustain it's false-positive rate.  Each time the 
	AMQ corrects a false-positive it must change its configuration.  Each configuration of the 
	AMQ for a given set $S$ can be defined using the list of false-positives that have been
	corrected.  This results in a one-to-one mapping between the set of fixed false-positives 
	and the configurations of the AMQ.  The number of configurations can then be lower 
	bounded by the number of sets of false-positives that the adversary can force the AMQ
	to fix.  The space required for the AMQ is thus lower bounded by the amount of space 
	required to enumerate its configurations.  Finally, the bound is shown to be tight by 
	providing a construction on an AMQ, the Broom Filter, that uses $O(\min \{n\log n, 
	n\log\log u\})$ bits of space with high probability. 
	
	


\end{document}

